{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nimport time\nfrom datetime import datetime\nfrom scipy import integrate, optimize\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# ML libraries\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\n\nfrom tensorflow.python.ops import variables\nfrom tensorflow.python.framework import ops\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nfrom fancyimpute import KNN \n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":67,"outputs":[{"output_type":"stream","text":"/kaggle/input/who-immunization-coverage/BCG.csv\n/kaggle/input/who-immunization-coverage/MCV1.csv\n/kaggle/input/who-immunization-coverage/ROTAC.csv\n/kaggle/input/who-immunization-coverage/Hib3.csv\n/kaggle/input/who-immunization-coverage/Pol3.csv\n/kaggle/input/who-immunization-coverage/HepB3.csv\n/kaggle/input/who-immunization-coverage/PCV3.csv\n/kaggle/input/who-immunization-coverage/DTP3.csv\n/kaggle/input/who-immunization-coverage/PAB.csv\n/kaggle/input/who-immunization-coverage/MCV2.csv\n/kaggle/input/covid19-global-forecasting-week-4/train.csv\n/kaggle/input/covid19-global-forecasting-week-4/submission.csv\n/kaggle/input/covid19-global-forecasting-week-4/test.csv\n/kaggle/input/covid19-country-data-wk3-release/Data Join - RELEASE.csv\n/kaggle/input/politics/politics_apr2020.csv\n/kaggle/input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv\n/kaggle/input/wdi-data-covid19/wdi_data.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# Helper functions for managing the data\n\ndef get_place(row):\n    place = row[2]\n    if isinstance(row[1], str):\n        place = row[1]\n    return place\n\n# Returns a dictionary, keyed by places, of their data\ndef separate_by_place(data):\n    place_data = {}\n    for row in data:\n        place = get_place(row)\n        if place in place_data:\n            place_data[place].append(row)\n        else:\n            place_data[place] = [row]\n    return place_data\n\ndef rmsle(y_test, predictions):\n    return np.sqrt(mean_squared_log_error(y_test, predictions))\n\n# Get dict of [cases_dom, deaths_dom, cases_int, deaths_int] for each country\ndef current_day_info(dataset, day):\n    day_data = {}\n    indices = np.where(dataset[:, 3] == day)\n    total_cases = np.sum(dataset[indices, 4])\n    total_deaths = np.sum(dataset[indices, 5])\n    for row in dataset[indices]:\n        place = get_place(row)\n        day_data[place] = [row[4], row[5], total_cases, total_deaths]\n    return day_data\n\n# Add previous day total cases, deaths domestically and internationally\ndef make_nn_train_data(dataset):\n    # Create the columns to fill\n    added_data = np.c_[dataset, np.zeros(len(dataset))]\n    added_data = np.c_[added_data, np.zeros(len(dataset))]\n    added_data = np.c_[added_data, np.zeros(len(dataset))]\n    added_data = np.c_[added_data, np.zeros(len(dataset))]\n    \n    # For each day in data set, and each country, grab [cases_dom, deaths_dom, cases_int, deaths_int]\n    data_day_place = []\n    num_days = np.amax(dataset[:, 3])\n    for day in range(int(num_days)):\n        data_day_place.append(current_day_info(dataset, day))\n        \n    # Now insert into the dataset\n    for index in range(len(added_data)):\n        row = dataset[index]\n        place = get_place(row)\n        prev_day = int(row[3] - 1)\n        if prev_day >= 0:\n            added_data[index, [-4, -3, -2, -1]] = data_day_place[prev_day][place]\n        else:\n            added_data[index, [-4, -3, -2, -1]] = [0.0, 0.0, 0.0, 0.0]\n            \n    x_indices = [3] + [i for i in range(6, len(added_data[0]))]\n    y_indices = [4, 5]\n    id_indices = [0, 1, 2]\n    \n    train_x = added_data[:, x_indices]\n    train_y = added_data[:, y_indices]\n    train_id = added_data[:, id_indices]\n    \n    # Change y to be delta cases and deaths. Spots -4, -3 of x are already domestic cases, deaths previously.\n    # And y is how many occur by the end of the day, so take difference\n    train_y = train_y - train_x[:, [-4, -3]]\n    \n    return train_x, train_y, train_id","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load Data\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\ntest_norm = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\n\n# First, add some data (first WDI obtained here https://www.kaggle.com/sambitmukherjee/covid-19-data-adding-world-development-indicators/output)\nwdi_info = pd.read_csv(\"../input/wdi-data-covid19/wdi_data.csv\")\ntrain_wdi = pd.merge(train, wdi_info,  how='left', on=['Province_State','Country_Region'])\n\n# Now add health system data\nhealth_info = pd.read_csv(\"../input/world-bank-wdi-212-health-systems/2.12_Health_systems.csv\")\ntrain_wdi = pd.merge(train_wdi, health_info,  how='left', on=['Province_State','Country_Region'])\n\n# Add personality info\npersonality_info = pd.read_csv(\"../input/covid19-country-data-wk3-release/Data Join - RELEASE.csv\")\npersonality_info = personality_info.rename(columns={\"TRUE POPULATION\": \"TRUE_POPULATION\"})\npersonality_info.pct_in_largest_city = personality_info.pct_in_largest_city.apply(lambda x: x.replace('%', ''))\npersonality_info.TRUE_POPULATION = personality_info.TRUE_POPULATION.apply(lambda x: x.replace(',', ''))\ntrain_wdi = pd.merge(train_wdi, personality_info,  how='left', on=['Province_State','Country_Region'])\n\n# Add leader info https://www.kaggle.com/lunatics/global-politcs-and-governance-data-apr-2020\nleader_info = pd.read_csv(\"../input/politics/politics_apr2020.csv\")\ntrain_wdi = pd.merge(train_wdi, leader_info,  how='left', on=['Country_Region'])\n\n# Add immunization coverage https://www.kaggle.com/lsind18/who-immunization-coverage\nfor filename in os.listdir(\"../input/who-immunization-coverage\"):\n    immun_info = pd.read_csv(\"../input/who-immunization-coverage/\" + filename).iloc[:,0:2]\n    immun_info = immun_info.rename(columns={\"Country\": \"Country_Region\", \"2018\": filename})\n    train_wdi = pd.merge(train_wdi, immun_info,  how='left', on=['Country_Region'])\n\n# Replace bad data with nan\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#NULL!', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#DIV/0!', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('#N/A', np.nan))\ntrain_wdi = train_wdi.apply(lambda x: x.replace('N.A.', np.nan))\ntrain_wdi = train_wdi.drop(['World_Bank_Name'], axis=1)\n\n# Convert dates to integers, starting from 0\ntest_norm[\"Date\"] = (pd.to_datetime(test_norm['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\ntrain_wdi[\"Date\"] = (pd.to_datetime(train_wdi['Date']) - pd.to_datetime(min(train_wdi['Date']))).dt.days\ntrain[\"Date\"] = (pd.to_datetime(train['Date']) - pd.to_datetime(min(train['Date']))).dt.days\n\ntrain = train.to_numpy()\ntrain_wdi = train_wdi.to_numpy()\ntest_norm = test_norm.to_numpy()\n\n# Cast to float\nindices = [i for i in range(3, len(train_wdi[0]))]\ntrain_wdi[:, indices] = train_wdi[:, indices].astype('float64') ","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply k nearest neighbors to obtain data for nan\ntrain_wdi[:, indices] = KNN(k=5).fit_transform(train_wdi[:, indices])","execution_count":70,"outputs":[{"output_type":"stream","text":"Imputing row 1/24414 with 8 missing, elapsed time: 360.888\nImputing row 101/24414 with 7 missing, elapsed time: 360.960\nImputing row 201/24414 with 9 missing, elapsed time: 361.031\nImputing row 301/24414 with 14 missing, elapsed time: 361.146\nImputing row 401/24414 with 13 missing, elapsed time: 361.244\nImputing row 501/24414 with 2 missing, elapsed time: 361.330\nImputing row 601/24414 with 8 missing, elapsed time: 361.379\nImputing row 701/24414 with 13 missing, elapsed time: 361.482\nImputing row 801/24414 with 13 missing, elapsed time: 361.596\nImputing row 901/24414 with 13 missing, elapsed time: 361.708\nImputing row 1001/24414 with 13 missing, elapsed time: 361.822\nImputing row 1101/24414 with 13 missing, elapsed time: 361.933\nImputing row 1201/24414 with 13 missing, elapsed time: 362.047\nImputing row 1301/24414 with 4 missing, elapsed time: 362.119\nImputing row 1401/24414 with 11 missing, elapsed time: 362.201\nImputing row 1501/24414 with 9 missing, elapsed time: 362.299\nImputing row 1601/24414 with 2 missing, elapsed time: 362.356\nImputing row 1701/24414 with 11 missing, elapsed time: 362.424\nImputing row 1801/24414 with 2 missing, elapsed time: 362.483\nImputing row 1901/24414 with 10 missing, elapsed time: 362.523\nImputing row 2001/24414 with 11 missing, elapsed time: 362.618\nImputing row 2101/24414 with 11 missing, elapsed time: 362.718\nImputing row 2201/24414 with 12 missing, elapsed time: 362.826\nImputing row 2301/24414 with 10 missing, elapsed time: 362.927\nImputing row 2401/24414 with 0 missing, elapsed time: 362.965\nImputing row 2501/24414 with 3 missing, elapsed time: 363.105\nImputing row 2601/24414 with 8 missing, elapsed time: 363.145\nImputing row 2701/24414 with 55 missing, elapsed time: 363.433\nImputing row 2801/24414 with 11 missing, elapsed time: 363.661\nImputing row 2901/24414 with 4 missing, elapsed time: 363.938\nImputing row 3001/24414 with 10 missing, elapsed time: 363.994\nImputing row 3101/24414 with 13 missing, elapsed time: 364.097\nImputing row 3201/24414 with 13 missing, elapsed time: 364.211\nImputing row 3301/24414 with 13 missing, elapsed time: 364.324\nImputing row 3401/24414 with 13 missing, elapsed time: 364.438\nImputing row 3501/24414 with 15 missing, elapsed time: 364.565\nImputing row 3601/24414 with 13 missing, elapsed time: 364.681\nImputing row 3701/24414 with 13 missing, elapsed time: 364.794\nImputing row 3801/24414 with 13 missing, elapsed time: 364.909\nImputing row 3901/24414 with 15 missing, elapsed time: 365.022\nImputing row 4001/24414 with 11 missing, elapsed time: 365.149\nImputing row 4101/24414 with 12 missing, elapsed time: 365.253\nImputing row 4201/24414 with 2 missing, elapsed time: 365.304\nImputing row 4301/24414 with 15 missing, elapsed time: 365.423\nImputing row 4401/24414 with 15 missing, elapsed time: 365.553\nImputing row 4501/24414 with 15 missing, elapsed time: 365.707\nImputing row 4601/24414 with 15 missing, elapsed time: 365.836\nImputing row 4701/24414 with 15 missing, elapsed time: 365.966\nImputing row 4801/24414 with 15 missing, elapsed time: 366.095\nImputing row 4901/24414 with 15 missing, elapsed time: 366.238\nImputing row 5001/24414 with 15 missing, elapsed time: 366.367\nImputing row 5101/24414 with 15 missing, elapsed time: 366.497\nImputing row 5201/24414 with 14 missing, elapsed time: 366.623\nImputing row 5301/24414 with 15 missing, elapsed time: 366.750\nImputing row 5401/24414 with 15 missing, elapsed time: 366.879\nImputing row 5501/24414 with 15 missing, elapsed time: 367.007\nImputing row 5601/24414 with 15 missing, elapsed time: 367.136\nImputing row 5701/24414 with 15 missing, elapsed time: 367.265\nImputing row 5801/24414 with 15 missing, elapsed time: 367.395\nImputing row 5901/24414 with 15 missing, elapsed time: 367.525\nImputing row 6001/24414 with 15 missing, elapsed time: 367.657\nImputing row 6101/24414 with 15 missing, elapsed time: 367.793\nImputing row 6201/24414 with 15 missing, elapsed time: 367.926\nImputing row 6301/24414 with 15 missing, elapsed time: 368.057\nImputing row 6401/24414 with 15 missing, elapsed time: 368.188\nImputing row 6501/24414 with 15 missing, elapsed time: 368.318\nImputing row 6601/24414 with 15 missing, elapsed time: 368.447\nImputing row 6701/24414 with 15 missing, elapsed time: 368.577\nImputing row 6801/24414 with 1 missing, elapsed time: 368.690\nImputing row 6901/24414 with 20 missing, elapsed time: 368.761\nImputing row 7001/24414 with 44 missing, elapsed time: 369.080\nImputing row 7101/24414 with 44 missing, elapsed time: 369.208\nImputing row 7201/24414 with 5 missing, elapsed time: 369.536\nImputing row 7301/24414 with 11 missing, elapsed time: 369.606\nImputing row 7401/24414 with 4 missing, elapsed time: 369.662\nImputing row 7501/24414 with 17 missing, elapsed time: 369.732\nImputing row 7601/24414 with 17 missing, elapsed time: 369.880\nImputing row 7701/24414 with 7 missing, elapsed time: 369.980\nImputing row 7801/24414 with 10 missing, elapsed time: 370.383\nImputing row 7901/24414 with 13 missing, elapsed time: 370.478\nImputing row 8001/24414 with 9 missing, elapsed time: 370.581\nImputing row 8101/24414 with 3 missing, elapsed time: 370.628\nImputing row 8201/24414 with 2 missing, elapsed time: 370.690\nImputing row 8301/24414 with 13 missing, elapsed time: 370.742\nImputing row 8401/24414 with 12 missing, elapsed time: 370.855\nImputing row 8501/24414 with 2 missing, elapsed time: 370.895\nImputing row 8601/24414 with 8 missing, elapsed time: 371.161\nImputing row 8701/24414 with 8 missing, elapsed time: 371.235\nImputing row 8801/24414 with 3 missing, elapsed time: 371.279\nImputing row 8901/24414 with 13 missing, elapsed time: 371.387\nImputing row 9001/24414 with 14 missing, elapsed time: 371.500\nImputing row 9101/24414 with 14 missing, elapsed time: 371.620\nImputing row 9201/24414 with 14 missing, elapsed time: 371.740\nImputing row 9301/24414 with 14 missing, elapsed time: 371.861\nImputing row 9401/24414 with 14 missing, elapsed time: 371.982\nImputing row 9501/24414 with 69 missing, elapsed time: 372.614\nImputing row 9601/24414 with 3 missing, elapsed time: 372.866\nImputing row 9701/24414 with 13 missing, elapsed time: 372.920\nImputing row 9801/24414 with 9 missing, elapsed time: 373.020\nImputing row 9901/24414 with 6 missing, elapsed time: 373.084\nImputing row 10001/24414 with 9 missing, elapsed time: 373.122\nImputing row 10101/24414 with 3 missing, elapsed time: 373.183\nImputing row 10201/24414 with 13 missing, elapsed time: 373.264\nImputing row 10301/24414 with 13 missing, elapsed time: 373.313\nImputing row 10401/24414 with 11 missing, elapsed time: 373.425\nImputing row 10501/24414 with 8 missing, elapsed time: 373.511\nImputing row 10601/24414 with 9 missing, elapsed time: 373.591\nImputing row 10701/24414 with 8 missing, elapsed time: 374.005\nImputing row 10801/24414 with 4 missing, elapsed time: 374.067\nImputing row 10901/24414 with 10 missing, elapsed time: 374.136\nImputing row 11001/24414 with 3 missing, elapsed time: 374.163\nImputing row 11101/24414 with 12 missing, elapsed time: 374.210\nImputing row 11201/24414 with 7 missing, elapsed time: 374.300\nImputing row 11301/24414 with 4 missing, elapsed time: 374.346\nImputing row 11401/24414 with 3 missing, elapsed time: 374.367\nImputing row 11501/24414 with 6 missing, elapsed time: 374.404\nImputing row 11601/24414 with 3 missing, elapsed time: 374.443\nImputing row 11701/24414 with 3 missing, elapsed time: 374.500\nImputing row 11801/24414 with 9 missing, elapsed time: 374.540\nImputing row 11901/24414 with 11 missing, elapsed time: 374.631\nImputing row 12001/24414 with 30 missing, elapsed time: 374.842\nImputing row 12101/24414 with 10 missing, elapsed time: 374.908\nImputing row 12201/24414 with 19 missing, elapsed time: 375.026\nImputing row 12301/24414 with 3 missing, elapsed time: 375.123\nImputing row 12401/24414 with 11 missing, elapsed time: 375.206\nImputing row 12501/24414 with 18 missing, elapsed time: 375.311\nImputing row 12601/24414 with 28 missing, elapsed time: 375.510\nImputing row 12701/24414 with 2 missing, elapsed time: 375.612\nImputing row 12801/24414 with 55 missing, elapsed time: 375.684\nImputing row 12901/24414 with 10 missing, elapsed time: 376.073\nImputing row 13001/24414 with 66 missing, elapsed time: 376.590\n","name":"stdout"},{"output_type":"stream","text":"Imputing row 13101/24414 with 3 missing, elapsed time: 376.854\nImputing row 13201/24414 with 9 missing, elapsed time: 376.941\nImputing row 13301/24414 with 4 missing, elapsed time: 377.004\nImputing row 13401/24414 with 11 missing, elapsed time: 377.082\nImputing row 13501/24414 with 2 missing, elapsed time: 377.166\nImputing row 13601/24414 with 17 missing, elapsed time: 377.222\nImputing row 13701/24414 with 13 missing, elapsed time: 377.359\nImputing row 13801/24414 with 4 missing, elapsed time: 377.418\nImputing row 13901/24414 with 1 missing, elapsed time: 377.494\nImputing row 14001/24414 with 9 missing, elapsed time: 377.532\nImputing row 14101/24414 with 3 missing, elapsed time: 377.584\nImputing row 14201/24414 with 14 missing, elapsed time: 377.658\nImputing row 14301/24414 with 69 missing, elapsed time: 378.002\nImputing row 14401/24414 with 14 missing, elapsed time: 378.561\nImputing row 14501/24414 with 14 missing, elapsed time: 378.680\nImputing row 14601/24414 with 2 missing, elapsed time: 378.713\nImputing row 14701/24414 with 8 missing, elapsed time: 378.752\nImputing row 14801/24414 with 9 missing, elapsed time: 378.832\nImputing row 14901/24414 with 18 missing, elapsed time: 378.908\nImputing row 15001/24414 with 4 missing, elapsed time: 379.042\nImputing row 15101/24414 with 4 missing, elapsed time: 379.079\nImputing row 15201/24414 with 2 missing, elapsed time: 379.104\nImputing row 15301/24414 with 12 missing, elapsed time: 379.148\nImputing row 15401/24414 with 8 missing, elapsed time: 379.246\nImputing row 15501/24414 with 0 missing, elapsed time: 379.280\nImputing row 15601/24414 with 2 missing, elapsed time: 379.288\nImputing row 15701/24414 with 4 missing, elapsed time: 379.310\nImputing row 15801/24414 with 3 missing, elapsed time: 379.342\nImputing row 15901/24414 with 4 missing, elapsed time: 379.376\nImputing row 16001/24414 with 8 missing, elapsed time: 379.466\nImputing row 16101/24414 with 36 missing, elapsed time: 379.627\nImputing row 16201/24414 with 35 missing, elapsed time: 379.969\nImputing row 16301/24414 with 36 missing, elapsed time: 380.313\nImputing row 16401/24414 with 66 missing, elapsed time: 380.599\nImputing row 16501/24414 with 10 missing, elapsed time: 381.153\nImputing row 16601/24414 with 9 missing, elapsed time: 381.238\nImputing row 16701/24414 with 9 missing, elapsed time: 381.278\nImputing row 16801/24414 with 21 missing, elapsed time: 381.394\nImputing row 16901/24414 with 3 missing, elapsed time: 381.499\nImputing row 17001/24414 with 3 missing, elapsed time: 381.525\nImputing row 17101/24414 with 19 missing, elapsed time: 381.600\nImputing row 17201/24414 with 1 missing, elapsed time: 381.705\nImputing row 17301/24414 with 69 missing, elapsed time: 382.299\nImputing row 17401/24414 with 11 missing, elapsed time: 382.486\nImputing row 17501/24414 with 9 missing, elapsed time: 382.585\nImputing row 17601/24414 with 8 missing, elapsed time: 382.680\nImputing row 17701/24414 with 3 missing, elapsed time: 382.720\nImputing row 17801/24414 with 26 missing, elapsed time: 382.790\nImputing row 17901/24414 with 45 missing, elapsed time: 383.106\nImputing row 18001/24414 with 17 missing, elapsed time: 383.371\nImputing row 18101/24414 with 11 missing, elapsed time: 383.431\nImputing row 18201/24414 with 10 missing, elapsed time: 383.528\nImputing row 18301/24414 with 6 missing, elapsed time: 383.602\nImputing row 18401/24414 with 10 missing, elapsed time: 383.684\nImputing row 18501/24414 with 21 missing, elapsed time: 383.731\nImputing row 18601/24414 with 21 missing, elapsed time: 383.919\nImputing row 18701/24414 with 21 missing, elapsed time: 384.104\nImputing row 18801/24414 with 21 missing, elapsed time: 384.292\nImputing row 18901/24414 with 21 missing, elapsed time: 384.476\nImputing row 19001/24414 with 21 missing, elapsed time: 384.661\nImputing row 19101/24414 with 21 missing, elapsed time: 384.846\nImputing row 19201/24414 with 21 missing, elapsed time: 385.036\nImputing row 19301/24414 with 48 missing, elapsed time: 385.319\nImputing row 19401/24414 with 21 missing, elapsed time: 385.626\nImputing row 19501/24414 with 21 missing, elapsed time: 385.812\nImputing row 19601/24414 with 21 missing, elapsed time: 385.997\nImputing row 19701/24414 with 21 missing, elapsed time: 386.182\nImputing row 19801/24414 with 21 missing, elapsed time: 386.367\nImputing row 19901/24414 with 21 missing, elapsed time: 386.552\nImputing row 20001/24414 with 21 missing, elapsed time: 386.740\nImputing row 20101/24414 with 21 missing, elapsed time: 386.925\nImputing row 20201/24414 with 21 missing, elapsed time: 387.112\nImputing row 20301/24414 with 21 missing, elapsed time: 387.296\nImputing row 20401/24414 with 21 missing, elapsed time: 387.484\nImputing row 20501/24414 with 21 missing, elapsed time: 387.692\nImputing row 20601/24414 with 21 missing, elapsed time: 387.878\nImputing row 20701/24414 with 21 missing, elapsed time: 388.071\nImputing row 20801/24414 with 21 missing, elapsed time: 388.266\nImputing row 20901/24414 with 21 missing, elapsed time: 388.451\nImputing row 21001/24414 with 21 missing, elapsed time: 388.635\nImputing row 21101/24414 with 21 missing, elapsed time: 388.820\nImputing row 21201/24414 with 21 missing, elapsed time: 389.006\nImputing row 21301/24414 with 21 missing, elapsed time: 389.191\nImputing row 21401/24414 with 21 missing, elapsed time: 389.375\nImputing row 21501/24414 with 21 missing, elapsed time: 389.559\nImputing row 21601/24414 with 21 missing, elapsed time: 389.742\nImputing row 21701/24414 with 21 missing, elapsed time: 389.927\nImputing row 21801/24414 with 21 missing, elapsed time: 390.113\nImputing row 21901/24414 with 21 missing, elapsed time: 390.298\nImputing row 22001/24414 with 21 missing, elapsed time: 390.482\nImputing row 22101/24414 with 21 missing, elapsed time: 390.669\nImputing row 22201/24414 with 21 missing, elapsed time: 390.854\nImputing row 22301/24414 with 21 missing, elapsed time: 391.038\nImputing row 22401/24414 with 21 missing, elapsed time: 391.224\nImputing row 22501/24414 with 21 missing, elapsed time: 391.411\nImputing row 22601/24414 with 21 missing, elapsed time: 391.658\nImputing row 22701/24414 with 8 missing, elapsed time: 391.852\nImputing row 22801/24414 with 4 missing, elapsed time: 391.919\nImputing row 22901/24414 with 4 missing, elapsed time: 391.955\nImputing row 23001/24414 with 23 missing, elapsed time: 392.106\nImputing row 23101/24414 with 23 missing, elapsed time: 392.298\nImputing row 23201/24414 with 20 missing, elapsed time: 392.492\nImputing row 23301/24414 with 20 missing, elapsed time: 392.668\nImputing row 23401/24414 with 21 missing, elapsed time: 393.566\nImputing row 23501/24414 with 17 missing, elapsed time: 393.743\nImputing row 23601/24414 with 21 missing, elapsed time: 393.915\nImputing row 23701/24414 with 23 missing, elapsed time: 394.111\nImputing row 23801/24414 with 4 missing, elapsed time: 394.212\nImputing row 23901/24414 with 10 missing, elapsed time: 394.267\nImputing row 24001/24414 with 14 missing, elapsed time: 394.377\nImputing row 24101/24414 with 12 missing, elapsed time: 394.491\nImputing row 24201/24414 with 100 missing, elapsed time: 395.198\nImputing row 24301/24414 with 7 missing, elapsed time: 396.088\nImputing row 24401/24414 with 6 missing, elapsed time: 396.148\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create training sets\ntrain_x, train_y, train_info = make_nn_train_data(train_wdi)","execution_count":71,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Helper function to view gradients for debugging purposes\ndef debug_grads(sess, feed_dict):\n    var_list = (variables.trainable_variables() + ops.get_collection(\n        ops.GraphKeys.TRAINABLE_RESOURCE_VARIABLES))\n    print('variables')\n    for v in var_list:\n        print('  ', v.name)\n    # get all gradients\n    grads_and_vars = optimizer.compute_gradients(loss)\n    # train_op = optimizer.apply_gradients(grads_and_vars)\n\n    zipped_val = sess.run(grads_and_vars, feed_dict=feed_dict)\n\n    for rsl, tensor in zip(zipped_val, grads_and_vars):\n        print('-----------------------------------------')\n        print('name', tensor[0].name.replace('/tuple/control_dependency_1:0', '').replace('gradients/', ''))\n        print('gradient', rsl[0])\n        print('value', rsl[1])","execution_count":72,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create graph\n\ntf.reset_default_graph()\ngraph = tf.Graph()\n\nlearning_rate = 0.001\nNUM_FEATURES = train_x.shape[1]\n\nwith graph.as_default():\n    X = tf.placeholder(tf.float32, (None, NUM_FEATURES))\n    labels = tf.placeholder(tf.float32, (None, 2))\n    with tf.name_scope(\"fcn1\"):\n        W1 = tf.get_variable('W1', shape=(NUM_FEATURES, 500), initializer=tf.keras.initializers.glorot_normal())\n        b1 = tf.Variable(tf.zeros((500,)), trainable=True)\n        X1 = tf.add(tf.matmul(X, W1), b1)\n        X1 = tf.layers.batch_normalization(X1)\n        X1 = tf.nn.leaky_relu(X1)\n    with tf.name_scope(\"fcn2\"):\n        W2 = tf.get_variable('W2', shape=(500, 500), initializer=tf.keras.initializers.glorot_normal())\n        b2 = tf.Variable(tf.zeros((500,)), trainable=True)\n        X2 = tf.add(tf.matmul(X1, W2), b2)\n        X2 = tf.layers.batch_normalization(X2)\n        X2 = tf.nn.leaky_relu(X2)\n    with tf.name_scope(\"fcn3\"):\n        W3 = tf.get_variable('W3', shape=(500, 100), initializer=tf.keras.initializers.glorot_normal())\n        b3 = tf.Variable(tf.zeros((100,)), trainable=True)\n        X3 = tf.add(tf.matmul(X2, W3), b3)\n        X3 = tf.layers.batch_normalization(X3)\n        X3 = tf.nn.leaky_relu(X3)\n    with tf.name_scope(\"fcn4\"):\n        W4 = tf.get_variable('W4', shape=(100, 10), initializer=tf.keras.initializers.glorot_normal())\n        b4 = tf.Variable(tf.zeros((10,)), trainable=True)\n        X4 = tf.add(tf.matmul(X3, W4), b4)\n        X4 = tf.layers.batch_normalization(X4)\n        X4 = tf.nn.leaky_relu(X4)\n    with tf.name_scope(\"fcn5\"):\n        W5 = tf.get_variable('W5', shape=(10, 2), initializer=tf.keras.initializers.glorot_normal())\n        b5 = tf.Variable(tf.zeros((2,)), trainable=True)\n        predictions = tf.add(tf.matmul(X4, W5), b5)\n    loss = tf.losses.mean_squared_error(labels, predictions)\n    # loss = tf.math.sqrt(tf.math.reduce_mean(tf.math.square(tf.compat.v1.losses.log_loss(labels, predictions))))\n    \n    optimizer = tf.train.AdagradOptimizer(learning_rate)\n    train_op = optimizer.minimize(loss)\n    init = tf.global_variables_initializer()\n    saver = tf.train.Saver()","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Train\n\nNUM_EPOCHS = 50\nsave_freq = 100\nDEBUG = False\nlearning_rate = 0.007\nrestore = False\nsave = True\n\n# SKLearn scalers\nx_scaler = StandardScaler()\nx_scaler.fit(train_x)\ny_scaler = StandardScaler()\ny_scaler.fit(train_y)\n\n# K Fold CV\nkf = KFold(n_splits=10, shuffle=True)\n\nwith tf.Session(graph=graph) as sess:\n    if restore:\n        saver.restore(sess, \"tmp/model.ckpt\")\n        NUM_EPOCHS = 0\n    else:\n        init.run()\n        \n    for epoch in range(NUM_EPOCHS):\n        avg_loss = 0\n        kf.get_n_splits(train_x)\n        for train_indices, cv_indices in kf.split(train_x):\n            batch_x = train_x[train_indices]\n            batch_y = train_y[train_indices]\n            standardized_x = x_scaler.transform(batch_x)\n            standardized_y = y_scaler.transform(batch_y)\n            \n            cv_x = train_x[cv_indices]\n            cv_y = train_y[cv_indices]\n            cv_x = x_scaler.transform(cv_x)\n            cv_y = y_scaler.transform(cv_y)\n            cv_length = len(cv_indices)\n            \n            if DEBUG:\n                debug_grads(sess, feed_dict)\n                \n            feed_dict = {X: standardized_x, labels: standardized_y}\n            _, loss_val, outs = sess.run([train_op, loss, predictions], feed_dict=feed_dict)\n            avg_loss += loss_val\n            \n        feed_dict = {X: cv_x, labels: cv_y}\n        cv_loss = sess.run([loss], feed_dict=feed_dict)[0]\n        print(epoch, \"Avg Train Loss\", avg_loss/(len(train_x) - cv_length), \"Avg CV Loss\", cv_loss/len(cv_indices))\n        \n        # Save\n        if save and (epoch % save_freq == 0): \n            save_str = \"tmp/model\" + str(epoch) + \".ckpt\"\n            save_path = saver.save(sess, save_str)\n    \n    # Save final weights\n    if save: \n        save_path = saver.save(sess, \"tmp/model.ckpt\")","execution_count":74,"outputs":[{"output_type":"stream","text":"0 Avg Train Loss 0.00047232966307368077 Avg CV Loss 0.0004921713016014615\n1 Avg Train Loss 0.00045657169562263265 Avg CV Loss 0.0003994579492949502\n2 Avg Train Loss 0.000444021674946845 Avg CV Loss 0.00024262291639860122\n3 Avg Train Loss 0.0004326936973273518 Avg CV Loss 0.00030868674145033984\n4 Avg Train Loss 0.0004225889074945083 Avg CV Loss 0.00025027797437993685\n5 Avg Train Loss 0.0004131169473881314 Avg CV Loss 0.0005218591029797358\n6 Avg Train Loss 0.0004040059706751685 Avg CV Loss 0.0004069267458136284\n7 Avg Train Loss 0.0003951387292984072 Avg CV Loss 0.00040576690926213873\n8 Avg Train Loss 0.0003864299381337526 Avg CV Loss 0.0002866054304014313\n9 Avg Train Loss 0.00037783222107775944 Avg CV Loss 0.00032618384242106635\n10 Avg Train Loss 0.0003693718879401795 Avg CV Loss 0.0003444985354938062\n11 Avg Train Loss 0.00036109858258411915 Avg CV Loss 0.00033904677382847755\n12 Avg Train Loss 0.00035300112896914345 Avg CV Loss 0.0002425765219598166\n13 Avg Train Loss 0.00034514765211454386 Avg CV Loss 0.00028665446199778894\n14 Avg Train Loss 0.0003375599615749553 Avg CV Loss 0.0002908003911382104\n15 Avg Train Loss 0.0003302587357378134 Avg CV Loss 0.0003079810332001745\n16 Avg Train Loss 0.0003232396560942899 Avg CV Loss 0.0002410991521020348\n17 Avg Train Loss 0.0003165121381871881 Avg CV Loss 0.0002316281181141667\n18 Avg Train Loss 0.00031009434850747696 Avg CV Loss 0.00030614173788362135\n19 Avg Train Loss 0.0003039989631806302 Avg CV Loss 0.0003115829997549092\n20 Avg Train Loss 0.0002982221004314385 Avg CV Loss 0.000332573992656128\n21 Avg Train Loss 0.00029275946616475433 Avg CV Loss 0.0003019334936864744\n22 Avg Train Loss 0.0002876001149137925 Avg CV Loss 0.0002801146042342071\n23 Avg Train Loss 0.0002827404388790923 Avg CV Loss 0.000296540066923386\n24 Avg Train Loss 0.00027816725196051935 Avg CV Loss 0.0002535436545078601\n25 Avg Train Loss 0.00027386865927037774 Avg CV Loss 0.00027364675552324407\n26 Avg Train Loss 0.0002698296816593275 Avg CV Loss 0.00027167157333730333\n27 Avg Train Loss 0.0002660355407127353 Avg CV Loss 0.000302033266147987\n28 Avg Train Loss 0.0002624751987343562 Avg CV Loss 0.00026006463706029434\n29 Avg Train Loss 0.00025913573003664845 Avg CV Loss 0.0002390047117902919\n30 Avg Train Loss 0.0002560008045799475 Avg CV Loss 0.00015905079466551359\n31 Avg Train Loss 0.00025305306152474303 Avg CV Loss 0.00024313918882977128\n32 Avg Train Loss 0.0002502731950749732 Avg CV Loss 0.0002469116671561023\n33 Avg Train Loss 0.00024764975250182335 Avg CV Loss 0.00026944998784516494\n34 Avg Train Loss 0.0002451679987927592 Avg CV Loss 0.00021174253376230555\n35 Avg Train Loss 0.00024281407240248396 Avg CV Loss 0.00023419065389512065\n36 Avg Train Loss 0.00024057550879075552 Avg CV Loss 0.00016901142428225055\n37 Avg Train Loss 0.00023843766924344604 Avg CV Loss 0.00025380246222190124\n38 Avg Train Loss 0.00023638667322684065 Avg CV Loss 0.00019939414109914139\n39 Avg Train Loss 0.0002344162545629286 Avg CV Loss 0.00023638381668897204\n40 Avg Train Loss 0.00023252144371140005 Avg CV Loss 0.00020062165248594242\n41 Avg Train Loss 0.000230706618858969 Avg CV Loss 0.00026678543313513617\n42 Avg Train Loss 0.00022896685251110876 Avg CV Loss 0.0001259775056256861\n43 Avg Train Loss 0.00022729576998456165 Avg CV Loss 0.0002219972430756228\n44 Avg Train Loss 0.00022568422835595736 Avg CV Loss 0.00020668473413663533\n45 Avg Train Loss 0.00022412388628443734 Avg CV Loss 0.0002497441697345313\n46 Avg Train Loss 0.00022261156592258188 Avg CV Loss 0.00021966008659465936\n47 Avg Train Loss 0.00022114262867108423 Avg CV Loss 0.00019130657706676766\n48 Avg Train Loss 0.00021970970566736163 Avg CV Loss 0.0002548961500709343\n49 Avg Train Loss 0.00021831268026827356 Avg CV Loss 0.00021755404767322034\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"days_to_extend = 30\n\ndef row_to_nn(row, prev_day_data):\n    place = get_place(row)\n    new_row = np.copy(row)\n    new_row = np.append(new_row, prev_day_data[place])\n    x_indices = [3] + [i for i in range(6, len(new_row))]\n    return new_row[x_indices]\n\nlonger_train = np.copy(train_wdi)\n\nwith tf.Session(graph=graph) as sess:\n    saver.restore(sess, \"tmp/model.ckpt\")\n    num_days = int(np.amax(longer_train[:, 3]))\n    # x_indices = [3] + [i for i in range(6, len(added_data[0]))]\n    for day in range(num_days, num_days + days_to_extend):\n        print(day)\n        prev_day_data = current_day_info(longer_train, day)\n        indices = np.where(longer_train[:, 3] == day)\n        for row in longer_train[indices]:\n            # turn each item into nn data format\n            row_x = np.asarray([row_to_nn(row, prev_day_data)])\n            # Run through NN\n            standardized_x = x_scaler.transform(row_x)\n            feed_dict = {X: standardized_x}\n            outs = sess.run(predictions, feed_dict=feed_dict)\n            inverse_outs = y_scaler.inverse_transform(outs)[0]\n            # Floor at 0\n            if inverse_outs[0] < 0:\n                inverse_outs[0] = 0.0\n            if inverse_outs[1] < 0:\n                inverse_outs[1] = 0.0\n            # Create new row\n            new_row = np.copy(row)\n            new_row[3] += 1\n            new_row[4] += inverse_outs[0]\n            new_row[5] += inverse_outs[1]\n            longer_train = np.append(longer_train, [new_row], axis=0)","execution_count":101,"outputs":[{"output_type":"stream","text":"77\n78\n79\n80\n81\n82\n83\n84\n85\n86\n87\n88\n89\n90\n91\n92\n93\n94\n95\n96\n97\n98\n99\n100\n101\n102\n103\n104\n105\n106\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Convert date back\ntrain = pd.read_csv(\"../input/covid19-global-forecasting-week-4/train.csv\")\nstart_date = np.datetime64(np.min(train['Date']))\nnew_dates = []\nfor index in range(len(longer_train)):\n    new_date = start_date + np.timedelta64(int(longer_train[index][3]), 'D')\n    new_dates.append(new_date)\n\nconv_predictions = np.copy(longer_train)\nconv_predictions[:, 3] = new_dates\n\n# Save predictions as a file\nmy_columns = [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\", \"ProjectedCases\", \"Fatalities\"]\noutputs = conv_predictions[:, [0, 1, 2, 3, 4, 5]]\ndf = pd.DataFrame(outputs, columns=my_columns) \ndf.to_csv('predictions.csv', index=False)","execution_count":104,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create submission file\nsubmission = pd.read_csv(\"../input/covid19-global-forecasting-week-4/test.csv\")\nsubmission[\"Date\"] = pd.to_datetime(submission['Date'])\nsubmission = pd.merge(submission, df,  how='left', on=['Province_State', 'Country_Region', 'Date'])\nsubmission = submission[['ForecastId_x', 'ProjectedCases', 'Fatalities']]\nsubmission = submission.rename(columns={\"ForecastId_x\": \"ForecastId\", \"ProjectedCases\": \"ConfirmedCases\"})\nsubmission.to_csv('submission.csv', index=False)","execution_count":112,"outputs":[{"output_type":"stream","text":"   ForecastId_x Country_Region Province_State       Date ForecastId_y  \\\n0             1    Afghanistan            NaN 2020-04-02           72   \n1             2    Afghanistan            NaN 2020-04-03           73   \n2             3    Afghanistan            NaN 2020-04-04           74   \n3             4    Afghanistan            NaN 2020-04-05           75   \n4             5    Afghanistan            NaN 2020-04-06           76   \n\n  ProjectedCases Fatalities  \n0            273          6  \n1            281          6  \n2            299          7  \n3            349          7  \n4            367         11  \n","name":"stdout"},{"output_type":"error","ename":"KeyError","evalue":"('ForecastId_x', 'ProjectedCases', 'Fatalities')","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: ('ForecastId_x', 'ProjectedCases', 'Fatalities')","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-112-55c047b1e7a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Province_State'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Country_Region'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ForecastId_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ProjectedCases'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Fatalities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0msubmission\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"ForecastId_x\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ForecastId\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ProjectedCases\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"ConfirmedCases\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: ('ForecastId_x', 'ProjectedCases', 'Fatalities')"]}]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}